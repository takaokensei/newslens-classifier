"""
Data preparation module.
Handles data loading, splitting, preprocessing, and embedding generation.
"""
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, Optional
from sklearn.model_selection import train_test_split
import joblib

from src.config import DATA_CONFIG, FEATURE_CONFIG, PATHS
from src.preprocessing import preprocess_batch
from src.embeddings import generate_tfidf_embeddings, generate_bert_embeddings
from src.sanity_check import full_sanity_check


def load_raw_data(data_path: Optional[Path] = None) -> Tuple[pd.DataFrame, np.ndarray]:
    """
    Load raw data from CSV file.
    
    Args:
        data_path: Path to CSV file. If None, looks in data/raw/
    
    Returns:
        Tuple of (dataframe, labels array)
    """
    if data_path is None:
        # Look for CSV files in data/raw
        raw_dir = PATHS['data_raw']
        csv_files = list(raw_dir.glob('*.csv'))
        if not csv_files:
            raise FileNotFoundError(
                f"No CSV files found in {raw_dir}. "
                "Please place your dataset CSV file in data/raw/"
            )
        data_path = csv_files[0]
        print(f"ğŸ“‚ Loading data from: {data_path}")
    
    data_path = Path(data_path)
    if not data_path.exists():
        raise FileNotFoundError(f"Data file not found: {data_path}")
    
    # Try common column names for text and labels
    df = pd.read_csv(data_path)
    
    # Detect text column (common names: text, texto, content, noticia, etc.)
    text_cols = [col for col in df.columns if col.lower() in 
                 ['text', 'texto', 'content', 'noticia', 'news', 'article']]
    if not text_cols:
        # Assume first column is text
        text_col = df.columns[0]
        print(f"âš ï¸  No text column found, using first column: {text_col}")
    else:
        text_col = text_cols[0]
        print(f"âœ… Using text column: {text_col}")
    
    # Detect label column (common names: label, classe, category, etc.)
    label_cols = [col for col in df.columns if col.lower() in 
                  ['label', 'classe', 'class', 'category', 'categoria']]
    if not label_cols:
        # Assume last column is label
        label_col = df.columns[-1]
        print(f"âš ï¸  No label column found, using last column: {label_col}")
    else:
        label_col = label_cols[0]
        print(f"âœ… Using label column: {label_col}")
    
    texts = df[text_col].astype(str).values
    labels = df[label_col].values
    
    print(f"ğŸ“Š Loaded {len(texts)} samples with {len(np.unique(labels))} classes")
    
    return df, labels


def split_data(
    texts: np.ndarray,
    labels: np.ndarray,
    test_size: float = None,
    val_size: float = None,
    random_state: int = None
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Split data into train/validation/test sets with stratification.
    
    Args:
        texts: Array of texts
        labels: Array of labels
        test_size: Proportion for test set (default from config)
        val_size: Proportion for validation from remaining (default from config)
        random_state: Random state (default from config)
    
    Returns:
        Tuple of (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    config = DATA_CONFIG
    test_size = test_size or config['test_size']
    val_size = val_size or config['val_size']
    random_state = random_state or config['random_state']
    
    # First split: train+val (80%) vs test (20%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        texts,
        labels,
        test_size=test_size,
        stratify=labels if config['stratify'] else None,
        random_state=random_state
    )
    
    # Second split: train (75% of 80% = 60%) vs val (25% of 80% = 20%)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp,
        y_temp,
        test_size=val_size,
        stratify=y_temp if config['stratify'] else None,
        random_state=random_state
    )
    
    print(f"\nğŸ“Š Data Split:")
    print(f"  Train: {len(X_train)} samples ({len(X_train)/len(texts)*100:.1f}%)")
    print(f"  Validation: {len(X_val)} samples ({len(X_val)/len(texts)*100:.1f}%)")
    print(f"  Test: {len(X_test)} samples ({len(X_test)/len(texts)*100:.1f}%)")
    
    return X_train, X_val, X_test, y_train, y_val, y_test


def prepare_embeddings(
    X_train: np.ndarray,
    X_val: np.ndarray,
    X_test: np.ndarray,
    embedding_type: str = 'both',
    save: bool = True
) -> dict:
    """
    Generate embeddings for all splits.
    
    Args:
        X_train: Training texts
        X_val: Validation texts
        X_test: Test texts
        embedding_type: 'tfidf', 'bert', or 'both'
        save: Whether to save embeddings to disk
    
    Returns:
        Dictionary with embeddings and vectorizers/models
    """
    results = {}
    
    # Preprocess all texts
    print("\nğŸ”„ Preprocessing texts...")
    X_train_processed = preprocess_batch(X_train.tolist())
    X_val_processed = preprocess_batch(X_val.tolist())
    X_test_processed = preprocess_batch(X_test.tolist())
    
    if embedding_type in ['tfidf', 'both']:
        print("\nğŸ“Š Generating TF-IDF embeddings...")
        
        # Train TF-IDF on training data
        X_train_tfidf, vectorizer = generate_tfidf_embeddings(
            X_train_processed,
            save_path=PATHS['data_embeddings'] / 'tfidf_train.npz' if save else None,
            fit=True
        )
        
        # Transform validation and test
        X_val_tfidf, _ = generate_tfidf_embeddings(
            X_val_processed,
            save_path=PATHS['data_embeddings'] / 'tfidf_val.npz' if save else None,
            fit=False,
            vectorizer=vectorizer
        )
        
        X_test_tfidf, _ = generate_tfidf_embeddings(
            X_test_processed,
            save_path=PATHS['data_embeddings'] / 'tfidf_test.npz' if save else None,
            fit=False,
            vectorizer=vectorizer
        )
        
        results['tfidf'] = {
            'train': X_train_tfidf,
            'val': X_val_tfidf,
            'test': X_test_tfidf,
            'vectorizer': vectorizer
        }
        
        # Save vectorizer
        if save:
            vectorizer_path = PATHS['data_embeddings'] / 'tfidf_vectorizer.pkl'
            joblib.dump(vectorizer, vectorizer_path)
            print(f"âœ… Saved vectorizer to {vectorizer_path}")
    
    if embedding_type in ['bert', 'both']:
        print("\nğŸ§  Generating BERT embeddings...")
        print("   (This may take a while...)")
        
        # Generate BERT embeddings
        X_train_bert, model = generate_bert_embeddings(
            X_train_processed,
            save_path=PATHS['data_embeddings'] / 'bert_train.npy' if save else None,
            show_progress=True
        )
        
        X_val_bert, _ = generate_bert_embeddings(
            X_val_processed,
            save_path=PATHS['data_embeddings'] / 'bert_val.npy' if save else None,
            model=model,
            show_progress=True
        )
        
        X_test_bert, _ = generate_bert_embeddings(
            X_test_processed,
            save_path=PATHS['data_embeddings'] / 'bert_test.npy' if save else None,
            model=model,
            show_progress=True
        )
        
        results['bert'] = {
            'train': X_train_bert,
            'val': X_val_bert,
            'test': X_test_bert,
            'model': model
        }
    
    return results


def save_labels(y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray, save: bool = True):
    """Save labels to disk."""
    if save:
        np.save(PATHS['data_processed'] / 'labels_train.npy', y_train)
        np.save(PATHS['data_processed'] / 'labels_val.npy', y_val)
        np.save(PATHS['data_processed'] / 'labels_test.npy', y_test)
        print(f"\nâœ… Saved labels to {PATHS['data_processed']}")


def prepare_full_pipeline(
    data_path: Optional[Path] = None,
    embedding_type: str = 'both',
    save: bool = True
) -> dict:
    """
    Complete data preparation pipeline.
    
    Args:
        data_path: Path to raw data CSV
        embedding_type: 'tfidf', 'bert', or 'both'
        save: Whether to save intermediate files
    
    Returns:
        Dictionary with all prepared data
    """
    print("="*60)
    print("ğŸš€ Starting Data Preparation Pipeline")
    print("="*60)
    
    # 1. Load raw data
    df, labels = load_raw_data(data_path)
    
    # 2. Split data
    X_train, X_val, X_test, y_train, y_val, y_test = split_data(
        df.iloc[:, 0].values,  # Assuming first column is text
        labels
    )
    
    # 3. Generate embeddings
    embeddings = prepare_embeddings(
        X_train, X_val, X_test,
        embedding_type=embedding_type,
        save=save
    )
    
    # 4. Save labels
    save_labels(y_train, y_val, y_test, save=save)
    
    # 5. Sanity checks
    print("\nğŸ” Running sanity checks...")
    if 'tfidf' in embeddings:
        full_sanity_check(embeddings['tfidf']['train'], y_train, "TF-IDF Train")
        full_sanity_check(embeddings['tfidf']['val'], y_val, "TF-IDF Validation")
        full_sanity_check(embeddings['tfidf']['test'], y_test, "TF-IDF Test")
    
    if 'bert' in embeddings:
        full_sanity_check(embeddings['bert']['train'], y_train, "BERT Train")
        full_sanity_check(embeddings['bert']['val'], y_val, "BERT Validation")
        full_sanity_check(embeddings['bert']['test'], y_test, "BERT Test")
    
    print("\n" + "="*60)
    print("âœ… Data Preparation Complete!")
    print("="*60)
    
    return {
        'embeddings': embeddings,
        'labels': {
            'train': y_train,
            'val': y_val,
            'test': y_test
        },
        'texts': {
            'train': X_train,
            'val': X_val,
            'test': X_test
        }
    }

