# NewsLens AI Classifier

Production-grade text classifier benchmarking **Sparse (TF-IDF)** vs. **Dense (BERT)** embeddings. Focus on **Cold Start**, **Latency** & **Semantic Performance** trade-offs using **SVM/XGBoost**.

## ğŸ¯ Project Overview

**NewsLens AI** is a comparative analysis system for text classification that evaluates the trade-off between semantic performance (BERT) and computational efficiency (TF-IDF) for news classification tasks.

### Key Features

- **Dual Embedding Strategy**: TF-IDF (sparse) + BERT (dense) via sentence-transformers
- **Multiple Classifiers**: SVM (linear) and XGBoost
- **Production-Ready**: Logging, monitoring, and Streamlit interface
- **LLM Integration**: Groq API for class profiling and error analysis
- **Comprehensive Evaluation**: Accuracy, F1-macro, F1 per class, confusion matrices

## ğŸ“‹ Requirements

- Python 3.8+
- See `requirements.txt` for full dependency list

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/takaokensei/newslens-classifier.git
cd newslens-classifier

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Set up environment variables:

```bash
export GROQ_API_KEY=your_api_key_here
```

### Running the Streamlit App

```bash
streamlit run apps/app_streamlit.py
```

## ğŸ“ Project Structure

```
newslens-classifier/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original news dataset (6 classes)
â”‚   â”œâ”€â”€ processed/        # Preprocessed data
â”‚   â”œâ”€â”€ embeddings/       # Saved embeddings (.npz for TF-IDF, .npy for BERT)
â”‚   â””â”€â”€ novos/            # New texts for production simulation
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ predicoes.csv     # Prediction logs
â”œâ”€â”€ models/               # Trained models (.pkl or .joblib)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py         # Centralized configurations
â”‚   â”œâ”€â”€ preprocessing.py  # Unified preprocessing function
â”‚   â”œâ”€â”€ data_loader.py    # Polymorphic data loading
â”‚   â”œâ”€â”€ embeddings.py    # Embedding generation (TF-IDF and BERT)
â”‚   â”œâ”€â”€ train.py          # Training scripts
â”‚   â”œâ”€â”€ evaluate.py       # Evaluation and metrics
â”‚   â””â”€â”€ llm_analysis.py   # Groq API integration
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ processar_novos.py # Script to classify texts in data/novos/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ app_streamlit.py  # Main Streamlit application
â””â”€â”€ requirements.txt      # Project dependencies
```

## ğŸ”¬ Technical Details

### Embeddings

- **E1 - TF-IDF**: 20k features, unigrams + bigrams, sparse matrix (.npz)
- **E2 - BERT**: `neuralmind/bert-base-portuguese-cased`, mean pooling, dense array (.npy)

### Models

- **M1 - SVM**: Linear kernel, balanced class weights, probability=True
- **M2 - XGBoost**: 100 estimators, max_depth=6, parallel processing

### Data Split

- **Stratified split**: Train (60%) / Validation (20%) / Test (20%)
- Random state: 42 for reproducibility

## ğŸ“Š Evaluation Metrics

- Accuracy
- F1-Macro
- F1 per class
- Confusion matrices (4 combinations)
- Latency (ms/document)
- Cold start time
- Model size (MB)

## ğŸ“ License

MIT License

## ğŸ‘¤ Author

**CauÃ£ Vitor Figueredo Silva** - ELE 606 (UFRN) - Final Project

## ğŸ™ Acknowledgments

- Prof. JosÃ© Alfredo F. Costa (UFRN)
- NeuralMind for Portuguese BERT model
- Groq for LLM API access
