# NewsLens AI Classifier

Production-grade text classifier benchmarking **Sparse (TF-IDF)** vs. **Dense (BERT)** embeddings. Focus on **Cold Start**, **Latency** & **Semantic Performance** trade-offs using **SVM/XGBoost**.

## ğŸ¯ Project Overview

**NewsLens AI** is a comparative analysis system for text classification that evaluates the trade-off between semantic performance (BERT) and computational efficiency (TF-IDF) for news classification tasks in Portuguese.

### Key Features

- **Dual Embedding Strategy**: TF-IDF (sparse, 20k features) + BERT (dense, 768 dims) via sentence-transformers
- **Multiple Classifiers**: SVM (linear) and XGBoost with comprehensive evaluation
- **Production-Ready**: Complete logging system, monitoring dashboard, and Streamlit interface
- **LLM Integration**: Groq API (llama-3.3-70b-versatile) for class profiling and differential error analysis
- **Comprehensive Evaluation**: Accuracy, F1-macro, F1 per class, confusion matrices, latency, cold start
- **6 News Categories**: Economia, Esportes, PolÃ­cia e Direitos, PolÃ­tica, Turismo, Variedades e Sociedade

## ğŸ“‹ Requirements

- Python 3.8+
- See `requirements.txt` for full dependency list

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/takaokensei/newslens-classifier.git
cd newslens-classifier

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Create a `.env` file in the project root with your Groq API key:

```bash
# .env
GROQ_API_KEY=your_api_key_here
```

Or set it as an environment variable:

```bash
# Linux/Mac
export GROQ_API_KEY=your_api_key_here

# Windows PowerShell
$env:GROQ_API_KEY="your_api_key_here"
```

**Note:** A `.env.example` file is provided as a template. Copy it to `.env` and add your API key.

### Running the Streamlit App

```bash
streamlit run apps/app_streamlit.py
```

**âš ï¸ Important Tip for Windows Users:**

Always use the virtual environment activated before running Streamlit:

```powershell
# Activate virtual environment first
.venv\Scripts\Activate.ps1
streamlit run apps/app_streamlit.py
```

Or use the full path to ensure the correct Python environment:

```powershell
.venv\Scripts\streamlit.exe run apps/app_streamlit.py
```

This ensures that Streamlit uses the correct Python environment with all dependencies installed.

## ğŸ“ Project Structure

```
newslens-classifier/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original news dataset (6 classes, 315 samples)
â”‚   â”œâ”€â”€ processed/        # Preprocessed data and labels
â”‚   â”œâ”€â”€ embeddings/       # Saved embeddings (.npz for TF-IDF, .npy for BERT)
â”‚   â””â”€â”€ novos/            # New texts for production simulation
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ predicoes.csv     # Prediction logs (timestamp, text, class, score, model)
â”œâ”€â”€ models/               # Trained models (.pkl), metrics, confusion matrices
â”‚   â”œâ”€â”€ *.pkl             # Trained models (tfidf_svm, tfidf_xgb, bert_svm, bert_xgb)
â”‚   â”œâ”€â”€ table_*.csv        # Performance tables
â”‚   â”œâ”€â”€ cm_*.png          # Confusion matrices
â”‚   â”œâ”€â”€ class_profiles.json    # LLM-generated class archetypes
â”‚   â””â”€â”€ differential_errors.json  # LLM analysis of model differences
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ relatorio.tex     # LaTeX report (10-20 pages)
â”‚   â””â”€â”€ prompt_gamma_ai.md # Presentation prompt for Gamma AI
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py         # Centralized configurations
â”‚   â”œâ”€â”€ preprocessing.py  # Unified preprocessing function
â”‚   â”œâ”€â”€ data_loader.py    # Polymorphic data loading
â”‚   â”œâ”€â”€ embeddings.py     # Embedding generation (TF-IDF and BERT)
â”‚   â”œâ”€â”€ train.py          # Training scripts
â”‚   â”œâ”€â”€ evaluate.py       # Evaluation and metrics
â”‚   â”œâ”€â”€ llm_analysis.py   # Groq API integration
â”‚   â”œâ”€â”€ logging_system.py # Prediction logging system
â”‚   â””â”€â”€ class_mapping.py  # Class to category mapping
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_phase2.py     # Phase 2: Training and evaluation
â”‚   â”œâ”€â”€ run_phase3.py     # Phase 3: LLM analysis and profiling
â”‚   â”œâ”€â”€ processar_novos.py # Production script for batch classification
â”‚   â””â”€â”€ test_production.py # Production environment validation
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ app_streamlit.py  # Main Streamlit application (classification + monitoring)
â”œâ”€â”€ .env                  # Environment variables (not in git)
â”œâ”€â”€ .env.example          # Environment variables template
â””â”€â”€ requirements.txt      # Project dependencies
```

## ğŸ”¬ Technical Details

### Embeddings

- **E1 - TF-IDF**: 20k features, unigrams + bigrams, sparse matrix (.npz)
- **E2 - BERT**: `neuralmind/bert-base-portuguese-cased`, mean pooling, dense array (.npy)

### Models

- **M1 - SVM**: Linear kernel, balanced class weights, probability=True
- **M2 - XGBoost**: 100 estimators, max_depth=6, parallel processing

### Data Split

- **Stratified split**: Train (60%) / Validation (20%) / Test (20%)
- Random state: 42 for reproducibility

## ğŸ“Š Results Summary

### Best Performance
- **BERT + SVM**: F1=1.0, Accuracy=1.0 (Perfect classification on test set)
- **TF-IDF + SVM**: F1=0.968, Accuracy=0.968 (96.8% of BERT performance)

### Efficiency Comparison
- **TF-IDF + SVM**: 0.14ms/doc, Cold Start: 0.08s, Size: 0.18 MB
- **BERT + SVM**: 0.12ms/doc, Cold Start: 2.23s, Size: 0.88 MB

### Key Findings
- BERT achieves perfect performance but has 28x longer cold start
- TF-IDF offers excellent efficiency with competitive performance (96.8%)
- SVM outperforms XGBoost in both embedding types
- BERT is essential for semantically ambiguous cases

## ğŸš€ Usage

### Training Models

```bash
# Phase 2: Train and evaluate models
python scripts/run_phase2.py

# Phase 3: Generate class profiles and error analysis
python scripts/run_phase3.py
```

### Production Classification

```bash
# Classify texts in data/novos/ directory
python scripts/processar_novos.py --model best

# Available models: best, tfidf_svm, tfidf_xgb, bert_svm, bert_xgb
```

### Streamlit Interface

The Streamlit app provides:
- **Classification Tab**: Real-time text classification with model selection
- **Monitoring Tab**: Dashboard with statistics, charts, and prediction logs
- **LLM Explanations**: AI-generated explanations for classifications (requires GROQ_API_KEY)

### Production Validation

```bash
# Run production environment tests
python scripts/test_production.py
```

## ğŸ“Š Evaluation Metrics

- **Accuracy**: Overall classification accuracy
- **F1-Macro**: Macro-averaged F1 score across all classes
- **F1 per class**: Individual F1 scores for each category
- **Confusion matrices**: Visual representation for all 4 model combinations
- **Latency**: Inference time per document (ms)
- **Cold start**: Model loading time (s)
- **Model size**: Disk space usage (MB)

## ğŸ“š Documentation

- **Report**: LaTeX report available in `reports/relatorio.tex` (compile with pdflatex)
- **Presentation**: Prompt for Gamma AI in `reports/prompt_gamma_ai.md`
- **Project Plan**: See `prompt_master.md` for complete project roadmap

## ğŸ”§ Development

### Project Phases

- âœ… **Phase 1**: Data preprocessing and embedding generation
- âœ… **Phase 2**: Model training and evaluation
- âœ… **Phase 3**: LLM analysis (class profiling, error analysis)
- âœ… **Phase 4**: Report writing, presentation, and final validation

### Testing

```bash
# Validate production environment
python scripts/test_production.py

# Quick model loading test
python -c "from src.train import load_trained_models; load_trained_models()"
```

## ğŸ“ License

MIT License

## ğŸ‘¤ Author

**CauÃ£ Vitor Figueredo Silva**  
ELE 606 - Final Project  
UFRN - Prof. JosÃ© Alfredo F. Costa

## ğŸ™ Acknowledgments

- **Prof. JosÃ© Alfredo F. Costa** (UFRN) - Project advisor
- **NeuralMind** - Portuguese BERT model (`neuralmind/bert-base-portuguese-cased`)
- **Groq** - LLM API access (llama-3.3-70b-versatile)
- **Streamlit** - Web application framework
