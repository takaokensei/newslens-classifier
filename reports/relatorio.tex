% !TEX program = pdflatex
% !TEX encoding = UTF-8

\documentclass[11pt,a4paper,twoside]{article}

%----------------------------------------------------------------------------------------
% SETUP DE IMAGENS
%----------------------------------------------------------------------------------------
\usepackage{graphicx} 
% Define que o LaTeX deve procurar imagens na pasta 'models' ou na raiz
\graphicspath{{./models/}{./}} 

%----------------------------------------------------------------------------------------
% ENCODING AND LANGUAGE
%----------------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazilian]{babel}
\usepackage[tracking=true]{microtype}

%----------------------------------------------------------------------------------------
% FONTS
%----------------------------------------------------------------------------------------
\usepackage{CormorantGaramond} 
\usepackage[defaultsans]{lato} 
\usepackage{FiraMono}

%----------------------------------------------------------------------------------------
% CORREÇÃO DE SCALING DE FONTE
%----------------------------------------------------------------------------------------
\usepackage{lmodern}      
\usepackage{anyfontsize}  

%----------------------------------------------------------------------------------------
% PACKAGES ESSENCIAIS
%----------------------------------------------------------------------------------------
\usepackage{amsmath, amsthm, mathtools, bm, amssymb}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{float}
\usepackage{eso-pic}
\usepackage[object=vectorian]{pgfornament}
\usepackage[outline]{contour}
\usepackage{url} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{multirow}

%----------------------------------------------------------------------------------------
% CODE LISTINGS
%----------------------------------------------------------------------------------------
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small\color{secondary},
    keywordstyle=\color{primary}\bfseries,
    stringstyle=\color{accent},
    commentstyle=\color{gray}\itshape,
    numberstyle=\tiny\color{gray},
    numbers=left,
    stepnumber=1,
    numbersep=8pt,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{lightgray},
    rulecolor=\color{primary},
    captionpos=b,
    abovecaptionskip=10pt,
    literate={á}{{\'a}}1 {ã}{{\~a}}1 {é}{{\'e}}1 {ç}{{\c{c}}}1
}

%----------------------------------------------------------------------------------------
% TIKZ SETUP & LAYERS
%----------------------------------------------------------------------------------------
\usepackage{tikz}
\usetikzlibrary{
    positioning, 
    shadows, 
    shadows.blur, 
    circuits.ee.IEC, 
    calc, 
    backgrounds, 
    shapes.geometric, 
    fit, 
    arrows.meta
}

\pgfdeclarelayer{background}
\pgfdeclarelayer{lines}
\pgfsetlayers{background,lines,main}

%----------------------------------------------------------------------------------------
% COLOR DEFINITIONS
%----------------------------------------------------------------------------------------
\definecolor{primary}{HTML}{1e3a8a}      
\definecolor{secondary}{HTML}{1f2937}    
\definecolor{accent}{HTML}{dc2626}        
\definecolor{lightgray}{HTML}{f8fafc}      
\definecolor{darkgray}{HTML}{374151}       
\definecolor{engineering}{HTML}{0369a1}    
\definecolor{lightblue}{HTML}{3b82f6}      
\definecolor{neuron}{RGB}{236, 72, 153}    
\definecolor{synapse}{RGB}{99, 102, 241}  

% Cores do Diagrama
\definecolor{bgDeep}{RGB}{248, 250, 252}
\definecolor{primaryDark}{RGB}{44, 62, 80}
\definecolor{accentTeal}{RGB}{22, 160, 133}
\definecolor{accentBlue}{RGB}{41, 128, 185}
\definecolor{accentPurple}{RGB}{142, 68, 173}
\definecolor{accentOrange}{RGB}{211, 84, 0}
\definecolor{alertRed}{RGB}{192, 57, 43}

%----------------------------------------------------------------------------------------
% BOXES AND ENVIRONMENTS
%----------------------------------------------------------------------------------------
\usepackage{tcolorbox}
\tcbuselibrary{theorems, skins, breakable}

\newtcolorbox{highlightbox}[1][]{
    enhanced, 
    colback=lightgray, 
    colframe=primary, 
    boxrule=1pt, 
    arc=5pt, 
    drop shadow={opacity=0.4, shadow xshift=2pt, shadow yshift=-2pt, fill=darkgray!80}, 
    breakable, 
    coltitle=white, 
    fonttitle=\bfseries\small\sffamily, 
    colbacktitle=primary, 
    attach boxed title to top left={xshift=12pt, yshift=-5pt}, 
    boxed title style={arc=3pt, boxrule=0.5pt, drop shadow={opacity=0.4, shadow xshift=1pt, shadow yshift=-1pt, fill=darkgray}}, 
    left=10pt, right=10pt, top=12pt, bottom=8pt, 
    #1
}

\newtcolorbox{futuristicbox}[1][]{
  enhanced, colback=secondary, colframe=primary!70, boxrule=0pt, arc=0pt,
  borderline west={1pt}{0pt}{primary!80},
  borderline east={1pt}{0pt}{accent!80},
  fontupper=\bfseries\color{white},
  halign=center, valign=center, boxsep=10pt, #1
}

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY AND REFERENCES
%----------------------------------------------------------------------------------------
\usepackage{csquotes}
\usepackage[
    backend=biber,
    style=abnt,           
    citestyle=authoryear, 
    sorting=nty,          
    url=true,             
    doi=true              
]{biblatex}

\addbibresource{referencias.bib}

%----------------------------------------------------------------------------------------
% LAYOUT SETTINGS
%----------------------------------------------------------------------------------------
\usepackage[a4paper,
    left=2.5cm, right=2.5cm,
    top=2.5cm, bottom=2.5cm,
    headheight=22pt, headsep=20pt,
    footskip=40pt]{geometry}

\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}

\contourlength{0.04em}
\newcommand*\splitdot{%
  \hspace{0.3em}%
  \raisebox{0.4ex}{%
    \tikz{%
      \fill[accent] (0,0) arc (90:270:0.4ex) -- cycle;%
      \fill[lightblue] (0,0) arc (90:-90:0.4ex) -- cycle;%
    }%
  }%
  \hspace{0.3em}~%
}
\newcommand{\sectiondivider}{\begin{center}\pgfornament[width=5cm, color=primary!50]{88}\end{center}}

\newcommand{\PageBorder}{%
\AddToShipoutPictureBG*{%
  \begin{tikzpicture}[remember picture, overlay]
    \draw[secondary, line width=0.4pt] ([xshift=1.5cm,yshift=-1.5cm]current page.north west) rectangle ([xshift=-1.5cm,yshift=1.5cm]current page.south east);
    \draw[primary!80, line width=0.8pt] ([xshift=1.6cm,yshift=-1.6cm]current page.north west) rectangle ([xshift=-1.6cm,yshift=1.6cm]current page.south east);
  \end{tikzpicture}}%
}
\newcommand{\NoPageBorder}{\ClearShipoutPictureBG}
\newcommand{\RestorePageBorder}{\PageBorder}

% Formatação de Seções
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries\color{primary}}{\thesection}{1em}{}[\vspace{-0.5em}\textcolor{accent}{\titlerule[0.8pt]}]
\titleformat{\subsection}{\large\bfseries\color{secondary}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\color{primary}}{\thesubsubsection}{1em}{}

% Hyperref
\usepackage[hidelinks, pdfencoding=auto, unicode]{hyperref}
\usepackage{cleveref}

% Styles
\usepackage{enumitem}
\usepackage{lettrine}
\setstretch{1.15} 
\pagestyle{fancy} \fancyhf{}
\fancyhead[LE,RO]{\small\color{primary}\textbf{\thepage}}
\fancyhead[RE]{\small\color{secondary}\textit{C. V. F. Silva}}
\fancyhead[LO]{\small\color{secondary}\textit{NewsLens AI: TF-IDF vs BERT}}
\fancyfoot[C]{\small\color{darkgray}Engenharia Elétrica – UFRN | 2025}
\renewcommand{\headrulewidth}{0.5pt} 

% Estilos TikZ do diagrama
\tikzset{
    complexNode/.style={draw=none, text=white, font=\bfseries\large, align=center, blur shadow={shadow blur steps=5, shadow xshift=2pt, shadow yshift=-2pt}, inner sep=6pt},
    terminal/.style={complexNode, rectangle, rounded corners=12pt, top color=accentTeal!80!black, bottom color=accentTeal, minimum width=4.5cm, minimum height=1.4cm},
    process/.style={complexNode, rectangle, rounded corners=4pt, top color=accentBlue!80!black, bottom color=accentBlue, minimum width=4.5cm, minimum height=1.3cm},
    decision/.style={complexNode, diamond, aspect=1.8, top color=accentOrange!80!black, bottom color=accentOrange, minimum width=4.0cm, font=\normalsize\bfseries\itshape},
    loopNode/.style={complexNode, regular polygon, regular polygon sides=6, shape border rotate=90, top color=accentPurple!80!black, bottom color=accentPurple, inner sep=-2pt, minimum width=3.0cm, font=\normalsize\bfseries},
    flowLine/.style={draw=primaryDark!80, line width=1.8pt, rounded corners=8pt, ->, >=LaTeX},
    feedbackLine/.style={flowLine, dashed, draw=alertRed, line width=1.3pt},
    clusterBox/.style={draw=primaryDark!40, dashed, line width=1.2pt, fill=bgDeep, inner sep=18pt, rounded corners=18pt, drop shadow={opacity=0.15}}
}

%========================================================================================
% DOCUMENT
%========================================================================================
\begin{document}

\NoPageBorder
\begin{titlepage}
\thispagestyle{empty}

% BACKGROUND GRAPHICS
\begin{tikzpicture}[remember picture, overlay]
    \fill[secondary] (current page.north west) rectangle (current page.south east);
    \fill[primary] (current page.north west) -- ($(current page.north west) + (0,-1cm)$) -- ($(current page.north east) + (-10cm,-2.5cm)$) -- (current page.north east) -- cycle;
    \fill[accent] (current page.south west) -- ($(current page.south west) + (12cm, 2cm)$) -- ($(current page.south east) + (0, 1cm)$) -- (current page.south east) -- cycle;
\end{tikzpicture}

\begin{center}
    \vspace*{3.5cm}
    \includegraphics[height=4.0cm]{ufrn_logo.png} 
    \vfill

    % --- IDENTIFICAÇÃO ---
    {\color{white}\large\bfseries\textls[150]{\MakeUppercase{Universidade Federal do Rio Grande do Norte}}}\\[0.2cm]
    
    % --- HIERARQUIA ---
    {\color{white}\bfseries\textls[100]{CENTRO DE TECNOLOGIA \\ DEPARTAMENTO DE ENGENHARIA ELÉTRICA}}\\[0.8cm]

    % --- MATÉRIA ---
    {\color{white}\bfseries\textls[50]{
        ELE 606 – \textcolor{white}{\MakeUppercase{Aprendizado de Máquina}}
    }}

    \vfill

    % --- TÍTULO ---
    \renewcommand{\LettrineFontHook}{\color{accent}\bfseries}
    {\color{white}\fontsize{24}{28}\selectfont\bfseries
    \lettrine[lines=2, lraise=0.1]{N}{EWSLENS AI} \\ 
    \textls[50]{\MakeUppercase{Análise Comparativa de Representações}}}
    
    \vfill
    
    % --- SUBTÍTULO ---
    \begin{futuristicbox}
    \textls[80]{SPARSE (TF-IDF) VS. DENSE (BERT) PARA CLASSIFICAÇÃO DE NOTÍCIAS}
    \end{futuristicbox}

    \vfill

    % --- AUTOR ---
    {\color{white}\Large\bfseries\textls[100]{\MakeUppercase{Cauã Vitor F. Silva}}}\\[0.3cm]
    {\color{white}\textls[50]{\MakeUppercase{Matrícula:} \texttt{20220014216} \splitdot ~ \MakeUppercase{Email:} \texttt{cauavitorfigueredo@gmail.com}}}\\[1.5cm]

    % --- PROFESSOR ---
    {\color{white}\small\bfseries\textls[50]{\MakeUppercase{Professor: \\ Prof. Dr. JOSÉ ALFREDO F. COSTA}}}
    
    \vfill
    
    % --- DATA ---
    {\color{white}\large\bfseries\textls[100]{\MakeUppercase{Natal-RN} \splitdot \today}}
    
    \vspace{2.5cm}
\end{center}
\end{titlepage}

%----------------------------------------------------------------------------------------
% EXECUTIVE SUMMARY (RESUMO)
%----------------------------------------------------------------------------------------
\RestorePageBorder
\pagestyle{fancy}
\pagenumbering{roman}
\setcounter{page}{1}
\thispagestyle{plain}
\begin{center}
    {\Large\bfseries\color{primary}Resumo}
\end{center}

\lettrine[lines=3, lhang=0.1, loversize=0.2]{\color{accent}\textbf{E}}{ste trabalho} apresenta uma análise comparativa entre representações esparsas (TF-IDF) e densas (BERT) para classificação de notícias em português. O sistema NewsLens AI foi desenvolvido para avaliar o trade-off entre performance semântica e eficiência computacional, utilizando modelos SVM e XGBoost. 

Os resultados demonstram que o BERT alcança performance superior (F1=1.0), enquanto o TF-IDF oferece melhor eficiência (0.14ms/doc) com performance competitiva (F1=0.97). O trabalho inclui integração com LLMs para perfilamento de classes e análise diferencial de erros, além de um sistema de produção completo com interface Streamlit e monitoramento.

\vspace{0.5cm}
\begin{center}
\begin{tikzpicture}
    \node[draw=primary, fill=lightgray, rounded corners=5pt, inner sep=8pt,
          drop shadow={opacity=0.4, shadow xshift=2pt, shadow yshift=-2pt, fill=darkgray!80}] {
    \begin{minipage}{0.8\textwidth}
        \centering
        \textbf{Palavras-chave:} NLP $\cdot$ TF-IDF $\cdot$ BERT $\cdot$ Classification $\cdot$ MLOps $\cdot$ Trade-off
    \end{minipage}
    };
\end{tikzpicture}
\end{center}
\newpage

%----------------------------------------------------------------------------------------
% ABSTRACT
%----------------------------------------------------------------------------------------
\RestorePageBorder
\thispagestyle{plain}
\begin{center}
    {\Large\bfseries\color{primary}Abstract}
\end{center}

\lettrine[lines=3, lhang=0.1, loversize=0.2]{\color{accent}\textbf{T}}{his work} presents a comparative analysis between sparse (TF-IDF) and dense (BERT) representations for Portuguese news classification. The NewsLens AI system was developed to evaluate the trade-off between semantic performance and computational efficiency, using SVM and XGBoost models.

Results demonstrate that BERT achieves superior performance (F1=1.0), while TF-IDF offers better efficiency (0.14ms/doc) with competitive performance (F1=0.97). The work includes LLM integration for class profiling and differential error analysis, as well as a complete production system with Streamlit interface and monitoring.

\vspace{0.5cm}
\begin{center}
\begin{tikzpicture}
     \node[draw=primary, fill=lightgray, rounded corners=5pt, inner sep=8pt,
          drop shadow={opacity=0.4, shadow xshift=2pt, shadow yshift=-2pt, fill=darkgray!80}] {
    \begin{minipage}{0.8\textwidth}
        \centering
        \textbf{Keywords:} NLP $\cdot$ TF-IDF $\cdot$ BERT $\cdot$ Classification $\cdot$ MLOps $\cdot$ Trade-off
    \end{minipage}
    };
\end{tikzpicture}
\end{center}
\newpage

%----------------------------------------------------------------------------------------
% PRELIMINARY PAGES
%----------------------------------------------------------------------------------------
\NoPageBorder
\pagestyle{fancy}

\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
% MAIN CONTENT
%----------------------------------------------------------------------------------------
\pagenumbering{arabic}
\setcounter{page}{1}

% ============================================================================
% 1. INTRODUÇÃO
% ============================================================================
\section{Introdução}

\subsection{Objetivo do Trabalho}
O objetivo deste trabalho é desenvolver e avaliar um sistema de classificação de notícias em português, comparando duas abordagens distintas de representação textual: representações esparsas (TF-IDF) e densas (BERT). O sistema \textit{NewsLens AI} foi projetado para quantificar o trade-off entre performance semântica e eficiência computacional em um ambiente de produção simulado.

\subsection{Hipótese Científica Central}

\begin{highlightbox}[title={Hipótese de Pesquisa}]
\textit{"O ganho semântico do BERT (Dense) justifica o aumento de latência e custo computacional em comparação a um TF-IDF (Sparse) bem ajustado para classificação de notícias?"}
\end{highlightbox}

Esta hipótese será testada através de métricas de performance (F1-Macro, Accuracy), eficiência (latência, cold start, tamanho do modelo) e análise qualitativa de casos onde os modelos diferem.

\subsection{Contexto e Motivação}
A classificação automática de textos é uma tarefa fundamental em processamento de linguagem natural (NLP). Com o advento de modelos de linguagem pré-treinados como BERT \parencite{devlin2019bert}, surgiu a necessidade de avaliar quando o ganho semântico justifica o custo computacional adicional em relação a métodos tradicionais como TF-IDF. 

Este trabalho contribui para essa discussão através de uma análise empírica rigorosa, utilizando uma base de dados real de notícias em português e métricas de engenharia de produção (latência, cold start, uso de memória).

\sectiondivider

% ============================================================================
% 2. BASE DE DADOS
% ============================================================================
\section{Descrição da Base de Dados}

\subsection{Características da Base}
A base de dados utilizada contém notícias em português classificadas em 6 categorias distintas. Após remoção de textos vazios, a base final contém 315 amostras válidas.

\begin{itemize}
    \item \textbf{Economia}: Notícias sobre economia, finanças e mercado.
    \item \textbf{Esportes}: Notícias esportivas.
    \item \textbf{Polícia e Direitos}: Notícias sobre segurança pública e direitos.
    \item \textbf{Política}: Notícias políticas.
    \item \textbf{Turismo}: Notícias sobre turismo e viagens.
    \item \textbf{Variedades e Sociedade}: Notícias gerais e sociais.
\end{itemize}

\begin{table}[H]
\centering
\caption{Distribuição de Classes na Base de Dados}
\label{tab:dataset_dist}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Categoria} & \textbf{Quantidade} & \textbf{Percentual} \\ \midrule
Economia & 53 & 16.8\% \\
Esportes & 55 & 17.5\% \\
Polícia e Direitos & 55 & 17.5\% \\
Política & 51 & 16.2\% \\
Turismo & 60 & 19.0\% \\
Variedades e Sociedade & 45 & 14.3\% \\ \midrule
\textbf{Total} & \textbf{315} & \textbf{100\%} \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Pré-processamento}
Foi aplicada uma função única (\texttt{preprocess\_text()}) em todo o pipeline:
\begin{enumerate}
    \item \textbf{Lowercase}: Conversão para minúsculas.
    \item \textbf{Limpeza}: Remoção de URLs, e-mails e espaços múltiplos.
    \item \textbf{Caracteres Especiais}: Mantidos acentos para preservar características do português.
\end{enumerate}

% ============================================================================
% 3. METODOLOGIA
% ============================================================================
\section{Métodos e Pipeline}

\subsection{Embeddings Utilizados}

\subsubsection{TF-IDF (Representação Esparsa)}
Implementado via \texttt{scikit-learn}:
\begin{itemize}
    \item \textbf{Features máximas}: 20.000.
    \item \textbf{N-gramas}: Unigramas e bigramas (1, 2).
    \item \textbf{Armazenamento}: Matriz esparsa comprimida (.npz), com densidade $\approx 1\%$.
\end{itemize}

\subsubsection{BERT (Representação Densa)}
Implementado via \texttt{sentence-transformers} \parencite{reimers2019sentence}:
\begin{itemize}
    \item \textbf{Modelo}: \texttt{neuralmind/bert-base-portuguese-cased} \parencite{souza2020bertimbau}.
    \item \textbf{Pooling}: Mean pooling.
    \item \textbf{Dimensão}: 768 features (float32).
\end{itemize}

\subsection{Modelos de Classificação e Otimização}

Utilizamos **SVM** (Support Vector Machine) \parencite{cortes1995support} e **XGBoost** \parencite{chen2016xgboost}. A otimização de hiperparâmetros foi realizada via **Optuna** (TPE Algorithm) com 50 trials.

\subsubsection{Espaço de Busca (Optuna)}

\textbf{SVM:}
\begin{itemize}
    \item \textbf{C}: 0.1 a 100.0 (escala logarítmica).
    \item \textbf{Kernel}: 'linear', 'rbf', 'poly'.
    \item \textbf{Gamma}: Coeficiente do kernel (para RBF/Poly).
\end{itemize}

\textbf{XGBoost:}
\begin{itemize}
    \item \textbf{Nº Estimadores}: 50 a 300.
    \item \textbf{Max Depth}: 3 a 10.
    \item \textbf{Learning Rate}: 0.01 a 0.3 (logarítmico).
    \item \textbf{Subsample/Colsample}: 0.6 a 1.0.
\end{itemize}

\begin{highlightbox}[title={Resultados da Otimização}]
A otimização gerou ganhos significativos, especialmente para o XGBoost:
\begin{itemize}
    \item \textbf{BERT + XGBoost:} +3.96\% em F1-Macro (Learning rate reduzido para 0.039).
    \item \textbf{BERT + SVM:} Seleção de kernel RBF (ao invés de Linear), indicando não-linearidade nos embeddings. O parâmetro C foi otimizado para 24.82.
\end{itemize}
\end{highlightbox}

\subsection{Estratégia de Validação}
Utilizou-se **K-Fold Cross-Validation Estratificado** ($K=5$) para garantir robustez estatística, resultando em desvios padrão baixos ($< 0.02$).

% ============================================================================
% 4. EXPERIMENTOS E RESULTADOS
% ============================================================================
\section{Experimentos e Resultados}

\subsection{Comparação: Modelos Padrão vs Otimizados}

A otimização bayesiana (Optuna) foi fundamental para maximizar o desempenho. A Tabela abaixo demonstra a comparação direta.

\begin{table}[H]
\centering
\caption{Comparação: Modelos Padrão vs Otimizados (K-fold CV, K=5)}
\label{tab:optimization_comparison}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Modelo} & \textbf{F1-Padrão} & \textbf{F1-Otimizado} & \textbf{Melhoria} & \textbf{Melhoria (\%)} \\ \midrule
TF-IDF + SVM & 0.9680 & 0.9682 & 0.0002 & 0.02\% \\
TF-IDF + XGBoost & 0.8478 & 0.8675 & 0.0197 & \textbf{2.32\%} \\
BERT + SVM & 0.9881 & 0.9918 & 0.0037 & 0.37\% \\
BERT + XGBoost & 0.9277 & 0.9645 & 0.0368 & \textbf{3.96\%} \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Eficiência e Performance Global}

A Tabela \ref{tab:efficiency} resume o trade-off central do trabalho. Os modelos foram avaliados com hiperparâmetros otimizados.

\begin{table}[H]
\centering
\caption{Eficiência e Performance Global dos Modelos (Otimizados)}
\label{tab:efficiency}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Setup} & \textbf{F1-Macro} & \textbf{Accuracy} & \textbf{Latência (ms)} & \textbf{Cold Start (s)} & \textbf{Tamanho (MB)} \\ \midrule
TF-IDF + SVM & 0.968 & 0.968 & \textbf{0.140} & \textbf{0.040} & \textbf{0.182} \\
TF-IDF + XGBoost & 0.697 & 0.714 & 0.370 & 0.060 & 0.489 \\
BERT + SVM & \textbf{1.000} & \textbf{1.000} & 0.160 & 0.620 & 0.875 \\
BERT + XGBoost & 0.967 & 0.968 & 0.390 & 0.550 & 0.428 \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Análise de Granularidade por Classe}

A performance por classe revela diferenças críticas entre as abordagens.

\begin{table}[H]
\centering
\caption{F1-Score por Classe e Modelo (Conjunto de Teste - Modelos Otimizados)}
\label{tab:classes}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Categoria} & \textbf{TF-IDF} & \textbf{TF-IDF} & \textbf{BERT} & \textbf{BERT} \\
 & \textbf{+SVM} & \textbf{+XGB} & \textbf{+SVM} & \textbf{+XGB} \\ \midrule
Economia & 0.952 & 0.571 & 1.000 & 1.000 \\
Esportes & 0.952 & 0.783 & 1.000 & 0.900 \\
Polícia e Direitos & 1.000 & 0.870 & 1.000 & 0.957 \\
Política & 1.000 & 0.870 & 1.000 & 1.000 \\
Turismo & 0.960 & 0.421 & 1.000 & 1.000 \\
Variedades & 0.941 & 0.667 & 1.000 & 0.947 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Análise Visual}

As Figuras abaixo ilustram as matrizes de confusão e o trade-off de eficiência.

% Aqui usei o caminho 'models/arquivo.png'
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{models/cm_tfidf_svm_test.png} 
\caption{TF-IDF + SVM}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{models/cm_bert_svm_test.png}
\caption{BERT + SVM}
\end{subfigure}
\caption{Matrizes de Confusão no Conjunto de Teste}
\label{fig:confusion}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{models/performance_efficiency_tradeoff.png}
    \caption{Trade-off: Performance (Eixo Y) vs Eficiência (Eixo X - Latência)}
    \label{fig:tradeoff}
\end{figure}

\textbf{Análise dos Dados:}
\begin{itemize}
    \item \textbf{Performance vs Latência:} O BERT+SVM oferece performance perfeita (F1=1.0) com latência de inferência similar ao TF-IDF (0.16ms vs 0.14ms).
    \item \textbf{Gargalo do Cold Start:} A grande diferença está na inicialização. O BERT requer 0.62s (após otimização), enquanto o TF-IDF é quase instantâneo (0.04s).
\end{itemize}

% ============================================================================
% 5. LLMS E ANÁLISE DE ERROS
% ============================================================================
\section{Uso de LLMs e Análise de Erros}

\subsection{Perfilamento de Classes}
Geramos "arquétipos" de classes usando uma abordagem híbrida:
\begin{itemize}
    \item \textbf{Chi-Squared (TF-IDF):} Top 20 tokens estatisticamente correlacionados.
    \item \textbf{Centroides (BERT):} 5 exemplos mais próximos do centro semântico da classe.
\end{itemize}

\subsection{Análise Diferencial de Erros (Groq API)}
Identificamos casos onde $(Pred_{BERT} = Correto) \land (Pred_{TFIDF} = Incorreto)$. Utilizando o modelo \texttt{llama-3.3-70b-versatile}, concluímos que o BERT supera o TF-IDF em:
\begin{enumerate}
    \item \textbf{Contexto Semântico:} Captura o tópico global mesmo sem palavras-chave óbvias.
    \item \textbf{Ambiguidade Lexical:} Diferencia sentidos de palavras polissêmicas (ex: "viagem" em turismo vs metafórico).
\end{enumerate}

% ============================================================================
% 6. SISTEMA DE PRODUÇÃO
% ============================================================================
\section{Sistema de Produção e Monitoramento}

\subsection{Arquitetura do Sistema Streamlit}

Desenvolvemos um sistema completo em Streamlit com duas abas principais para validação em ambiente real:

\begin{enumerate}
    \item \textbf{Classificação em Tempo Real}: 
    \begin{itemize}
        \item Seleção dinâmica de embedding (BERT/TF-IDF) e modelo (SVM/XGBoost).
        \item Exibição de score de confiança e distribuição de probabilidades.
        \item Integração opcional com LLM para gerar explicações da predição.
    \end{itemize}
    \item \textbf{Dashboard de Monitoramento}:
    \begin{itemize}
        \item Métricas agregadas e gráficos interativos (Plotly).
        \item Trade-off Performance vs Eficiência visualizado em tempo real.
        \item Distribuição de scores por modelo (Box Plot).
    \end{itemize}
\end{enumerate}

\subsection{Logging e Banco de Dados}
Todas as predições são persistidas em dois formatos para redundância e escalabilidade:
\begin{itemize}
    \item \textbf{CSV}: \texttt{logs/predicoes.csv} para auditoria rápida.
    \item \textbf{SQLite}: \texttt{logs/predicoes.db} para consultas estruturadas e alta performance.
\end{itemize}

Os logs capturam timestamp, texto (hash), classe predita, score de confiança e latência, permitindo o monitoramento de \textit{data drift} e performance em produção.

% ============================================================================
% 7. DISCUSSÃO
% ============================================================================
\section{Discussão}

\subsection{Comparação entre Embeddings}

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{TF-IDF (Esparso)} & \textbf{BERT (Denso)} \\ \midrule
+ Eficiência computacional extrema (0.14ms) & + Compreensão semântica profunda \\
+ Tamanho reduzido (0.18 MB) & + Performance superior (F1=1.0) \\
+ Alta interpretabilidade (tokens visíveis) & + Robustez a variações lexicais \\
- Não captura contexto semântico & - Cold Start significativo (2.23s original) \\
- Falha em ambiguidade lexical & - Maior uso de memória (0.88 MB) \\ \bottomrule
\end{tabular}
\end{table}

\subsection{SVM vs XGBoost}
O **SVM** demonstrou superioridade consistente sobre o XGBoost neste dataset (F1=0.968 vs 0.704 no TF-IDF). O kernel linear do SVM é matematicamente mais adequado para lidar com a alta dimensionalidade e esparsidade do TF-IDF, enquanto o XGBoost sofreu para encontrar cortes ótimos nas árvores de decisão com a base de dados limitada (315 amostras).

\subsection{Limitações e Trabalhos Futuros}
\begin{itemize}
    \item \textbf{Base de Dados:} 315 amostras podem não ser representativas de todas as nuances do português.
    \item \textbf{Overfitting:} O F1=1.0 do BERT+SVM pode indicar overfitting; testes em bases externas são necessários.
    \item \textbf{Expansão:} Coletar mais dados e testar ensembles combinando TF-IDF e BERT.
\end{itemize}

% ============================================================================
% 8. CONCLUSÃO
% ============================================================================
\section{Conclusão e Recomendações}

\subsection{Resposta à Hipótese}
A hipótese de que o BERT justifica seu custo depende do contexto:
\begin{itemize}
    \item \textbf{Sim}, para aplicações críticas onde cada erro tem alto custo (ganho de 3.2\% em F1 e robustez semântica).
    \item \textbf{Não}, para sistemas de alta escala/baixa latência, onde o TF-IDF+SVM entrega 96.8\% da performance com fração do custo de memória e inicialização instantânea.
\end{itemize}

\subsection{Contribuições}
Este trabalho entregou um sistema completo (\textit{NewsLens AI}) com interface Streamlit, monitoramento de logs, e uma análise rigorosa demonstrando que, embora representações densas sejam o estado da arte em performance, métodos clássicos bem otimizados permanecem altamente competitivos para classificação de notícias.

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\clearpage
\nocite{*} 
\printbibliography[heading=bibintoc, title={Referências Bibliográficas}]

% ============================================================================
% APÊNDICES
% ============================================================================
\newpage
\appendix
\section{Estrutura do Projeto}

% Usei +-- e | no lugar dos caracteres especiais para garantir que o LaTeX compile
\begin{lstlisting}
newslens-classifier/
+-- data/
|   +-- raw/              # Base original
|   +-- processed/        # Dados pre-processados
|   +-- embeddings/       # Embeddings salvos (.npy, .npz)
|   +-- novos/            # Novos textos para producao
+-- logs/
|   +-- predicoes.csv     # Log de predicoes
+-- models/               # Modelos treinados (.pkl)
+-- src/                  # Codigo fonte (preprocessing, training)
+-- scripts/              # Scripts de execucao e otimizacao
+-- apps/                 # Interface Streamlit
+-- reports/              # Relatorios e analises
\end{lstlisting}

\section{Fluxograma do Pipeline NewsLens}

\begin{figure}[H]
\centering
\tikzset{
    % Redefinindo estilos para ajuste local se necessário
}

\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[node distance=1.5cm and 2.0cm]

    % NÓS
    \node (input) [terminal] {Input Text\\(News)};
    \node (preproc) [process, below=of input] {Pré-processamento\\(Limpeza/Regex)};
    
    % Decisão Embedding
    \node (choice) [decision, below=of preproc] {Tipo Embedding?};
    
    % Caminho TF-IDF
    \node (tfidf) [process, below left=of choice, xshift=-1cm] {TF-IDF Vectorizer\\(Sparse)};
    \node (svm_tfidf) [process, below=of tfidf] {SVM (Linear)};
    
    % Caminho BERT
    \node (bert) [process, below right=of choice, xshift=1cm, top color=neuron, bottom color=synapse] {BERT Model\\(Dense Embeddings)};
    \node (svm_bert) [process, below=of bert] {SVM (RBF Kernel)};
    
    % Junção
    \node (output) [terminal, below=of choice, yshift=-7cm] {Classificação Final\\\& Confiança};
    
    % LLM Analysis
    \node (llm) [clusterBox, right=of bert, xshift=1cm, align=center, font=\small\bfseries] {LLM Analysis\\(Llama 3.3)};

    % ARESTAS
    \path [flowLine] (input) edge (preproc);
    \path [flowLine] (preproc) edge (choice);
    
    % Ramificação
    \draw [flowLine] (choice.west) -| node[above, xshift=-0.5cm] {Sparse} (tfidf.north);
    \draw [flowLine] (choice.east) -| node[above, xshift=0.5cm] {Dense} (bert.north);
    
    % Modelos
    \path [flowLine] (tfidf) edge (svm_tfidf);
    \path [flowLine] (bert) edge (svm_bert);
    
    % Saída
    \draw [flowLine] (svm_tfidf.south) |- (output.west);
    \draw [flowLine] (svm_bert.south) |- (output.east);
    
    % Conexão LLM (Feedback/Explicação)
    \draw [feedbackLine] (output.east) -| (llm.south);

    % LAYERS DE FUNDO
    \begin{pgfonlayer}{background}
        \node [clusterBox, fit=(input) (preproc), label={[anchor=south east, text=gray]north east:ETL}] {};
        \node [clusterBox, fit=(tfidf) (bert) (svm_tfidf) (svm_bert), label={[anchor=south east, text=gray]north east:MODELAGEM HÍBRIDA}] {};
    \end{pgfonlayer}

\end{tikzpicture}
}
\caption{Arquitetura do Sistema NewsLens AI.}
\label{fig:newslens-pipeline}
\end{figure}

\end{document}
